import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Ladda den tr√§nade modellen och tokenizer
model_path = "bert_router/checkpoint-984"  # üîπ √Ñndra till senaste checkpoint
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained("albert-base-v1")  

# Flytta till r√§tt enhet (GPU om tillg√§ngligt)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()  # S√§tt i inferensl√§ge

def classify_expert(question):
    """ Anv√§nd den tr√§nade ALBERT-modellen f√∂r att v√§lja expert. """
    inputs = tokenizer(question, return_tensors="pt").to(device)
    with torch.no_grad():  # Ingen gradientber√§kning beh√∂vs
        outputs = model(**inputs)

    # V√§lj experten med h√∂gsta sannolikhet
    predicted_expert = torch.argmax(outputs.logits, dim=-1).item()
    return predicted_expert

questions = [
    "What is 5+5?",  # Borde v√§lja matematik-experten
    "How do MOE work?",  # Programmeringsexpert
    "Who was Oprah Winfrey?",  # Faktaexpert
    "What is a remotecontrol?",  # Faktaexpert
    "What are the main causes of World War II?",  # Omskrivningsexpert
    "What are ?",  # Omskrivningsexpert
]

for q in questions:
    expert = classify_expert(q)
    print(f"üß† Question: {q}\nü§ñ Predicted Expert: {expert}\n")
